{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers # Getting ready to use a HF model.\n!pip install torch torchvision torchaudio # PyTorch install\n!pip install flash_attn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:17:13.673760Z","iopub.execute_input":"2025-01-14T18:17:13.673960Z","iopub.status.idle":"2025-01-14T18:17:40.531284Z","shell.execute_reply.started":"2025-01-14T18:17:13.673941Z","shell.execute_reply":"2025-01-14T18:17:40.530360Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nCollecting flash_attn\n  Downloading flash_attn-2.7.3.tar.gz (3.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash_attn) (2.4.1+cu121)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash_attn) (0.8.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash_attn) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash_attn) (1.3.0)\nBuilding wheels for collected packages: flash_attn\n  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flash_attn: filename=flash_attn-2.7.3-cp310-cp310-linux_x86_64.whl size=191333579 sha256=15aae40ce1b09f613ea943dd55f425c57e7496661b113fada685520d9339aea3\n  Stored in directory: /root/.cache/pip/wheels/85/d7/10/a74c9fe5ffe6ff306b27a220b2bf2f37d907b68fdcd138cdda\nSuccessfully built flash_attn\nInstalling collected packages: flash_attn\nSuccessfully installed flash_attn-2.7.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:17:59.231944Z","iopub.execute_input":"2025-01-14T18:17:59.232328Z","iopub.status.idle":"2025-01-14T18:18:00.774336Z","shell.execute_reply.started":"2025-01-14T18:17:59.232296Z","shell.execute_reply":"2025-01-14T18:18:00.773398Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Initial thought process\n\"\"\"\nCall a HF model -> pass through chat template -> function calling capabilities -> output\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:18:01.055120Z","iopub.execute_input":"2025-01-14T18:18:01.055607Z","iopub.status.idle":"2025-01-14T18:18:01.061445Z","shell.execute_reply.started":"2025-01-14T18:18:01.055573Z","shell.execute_reply":"2025-01-14T18:18:01.060581Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'\\nCall a HF model -> pass through chat template -> function calling capabilities -> output\\n'"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Stage 1: Research\nUpon any task involving open-source LLMs, the best thing to do is to do some research on the number of open-source LLMs available specific to your use-case. The best website for this is HuggingFace, which contains an innumerable number of repositories for LLM use-cases and modifications. \n\nTwo factors can mainly be kept in mind when it comes to finding suitable LLMs for any use-case. \n1. Size\n2. License\n\n### Size\nA company that is seriously delving into LLMs may have a cost requirement of using the model locally on the computer. This may be done by deploying a HuggingFace model onto SageMaker or a similar platform, and the LLM can be run locally for whatever use-case necessary. One has to be careful and take the size of the LLM (in billions of parameters) into account such that it doesn't exceed the cost threshold of the company. \n\n### License\nMany open-source LLMs may not be available for commercial use, though they might be available for anyone to use. Therefore, upon looking at any HuggingFace repository, it is important to look at the license file included. Good to go license files includes: \n1. MIT\n2. Apache\n3. BSD\n4. CC-by-NC (can be used but not for commercial)\n5. Llama (limited to llama family)\n\n","metadata":{}},{"cell_type":"markdown","source":"Upon researching, we will take a look at: \n1. OpenCodeInterpreter-DS-6.7B by m-a-p (multimodal art projection)\n2. DeepSeek-Coder-V2-Lite-Instruct by deepseek-ai\n3. Mistral-7B-Instruct-v0.3\n\nOne question that can be raised is why have I chosen instruct models rather than base. Instruct models have been fine-tuned to follow the question-answer format of a chatbot, unlike base models which is good for general language.\n\nThese models however, are definitely not comparable to GPT4 in terms of code generation benchmarks, however we cannot demonstrate that because of the closed source nature of GPT4. ","metadata":{}},{"cell_type":"code","source":"## OpenCodeInterpreter driver code (from HuggingFace)\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_path=\"m-a-p/OpenCodeInterpreter-DS-6.7B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:18:03.046506Z","iopub.execute_input":"2025-01-14T18:18:03.046913Z","iopub.status.idle":"2025-01-14T18:23:35.642153Z","shell.execute_reply.started":"2025-01-14T18:18:03.046875Z","shell.execute_reply":"2025-01-14T18:23:35.641430Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/5.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c5a73882a234e1a82be24a27509ad23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2d945df0ab04263a251040513791680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/462 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8f34705b9d4a5c9f750467160efc85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/716 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d88c05d787f043df87e8050a73b21280"}},"metadata":{}},{"name":"stderr","text":"Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93bab72be0dd4b968d87d2dfc99178f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9863b25ebb1d4bee8979215bbef3691f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c6adbb0d3747df97b4f6bad9e0cf34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04b184a2ce6a4614a5651196e0c7d2b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528e4e2a98f341d9a0f9c9292ad6c4f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6a3702762394375b430881e822ffffa"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32256, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Testing the code\nprompt = \"Write a function to find the shared elements from the given two lists.\"\ninputs = tokenizer.apply_chat_template(\n        [{'role': 'user', 'content': prompt }],\n        return_tensors=\"pt\"\n    ).to(model.device)\noutputs = model.generate(\n    inputs, \n    max_new_tokens=1024,\n    do_sample=False,\n    pad_token_id=tokenizer.eos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:23:35.643283Z","iopub.execute_input":"2025-01-14T18:23:35.643623Z","iopub.status.idle":"2025-01-14T18:23:46.171793Z","shell.execute_reply.started":"2025-01-14T18:23:35.643601Z","shell.execute_reply":"2025-01-14T18:23:46.170994Z"}},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"def shared_elements(list1, list2):\n    return list(set(list1) & set(list2))\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Getting the chat template for the model \nprint(inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:23:46.173388Z","iopub.execute_input":"2025-01-14T18:23:46.173942Z","iopub.status.idle":"2025-01-14T18:23:46.181739Z","shell.execute_reply.started":"2025-01-14T18:23:46.173908Z","shell.execute_reply":"2025-01-14T18:23:46.180886Z"}},"outputs":[{"name":"stdout","text":"tensor([[ 2042,   417,   274, 20926, 14244, 20391,    11, 26696,   254, 20676,\n         30742,   339,  8589,  2008,    11,  6908,   457, 20676, 30742,  7958,\n            11,   285,   340,   885,  3495,  4301,  4512,   276,  4531,  8214,\n            13,  1487,  4636,  2223, 13143,  4301,    11,  5411,   285, 13936,\n          4447,    11,   285,   746,  2159,    12, 13517,   250,  8214,  4301,\n            11,   340,   540, 20857,   276,  3495,    13,   185, 13518,  3649,\n          3475,    25,   185,  9083,   245,  1155,   276,  1273,   254,  7483,\n          4889,   473,   254,  2017,   979, 11996,    13,   185, 13518, 21289,\n            25,   185]], device='cuda:0')\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Stage 2: Function Calling and Code Generation\n\nWe will now come to the task given. I have chosen to demo two approaches: \n1. Using prompt engineering (by giving manual function calls)\n2. Using Open-AI style approach\n\n### Using prompt engineering\nWe will manually define `n` functions that the model can use. The limitation with this approach is that the functions called must be within the list defined. Any other functions that might have to be called will have to be implemented through either a fallback mechanism or fine-tune the model. ","metadata":{}},{"cell_type":"code","source":"# Creating our funciton caller \ndef function_caller(model_output):\n    try:\n        # Parse the model output\n        output = json.loads(model_output)\n        function_name = output[\"function_call\"]\n        arguments = output[\"arguments\"]\n\n        # Validate the function\n        if function_name not in functions:\n            raise ValueError(f\"Function '{function_name}' not found.\")\n\n        # Get the function and its signature\n        func = functions[function_name]\n        sig = inspect.signature(func)\n\n        # Validate the arguments\n        validated_args = {k: v for k, v in arguments.items() if k in sig.parameters}\n\n        # Call the function with validated arguments\n        result = func(**validated_args)\n        return result\n\n    except Exception as e:\n        return str(e)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Defining the prompt for our model. \nprompt = f\"\"\"\nYou are a helpful assistant that can call functions to perform various tasks.\n\nHere are the available functions you can call:\n\n1. `get_weather(location: str) -> str`: Returns the weather in the given location.\n2. `calculate_sum(a: int, b: int) -> int`: Returns the sum of two numbers.\n\nWhen you understand the user's request, return your response in the following JSON format:\n```json\n{{\n  \"function_call\": \"<function_name>\",\n  \"arguments\": {{\n    \"<argument1>\": <value1>,\n    \"<argument2>\": <value2>\n  }}\n}}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:33:12.344719Z","iopub.execute_input":"2025-01-14T18:33:12.345070Z","iopub.status.idle":"2025-01-14T18:33:12.349263Z","shell.execute_reply.started":"2025-01-14T18:33:12.345000Z","shell.execute_reply":"2025-01-14T18:33:12.348448Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Tokenize the prompt\ninputs = tokenizer.apply_chat_template(\n    [{'role': 'user', 'content': prompt}],\n    return_tensors=\"pt\"\n).to(model.device)\n\n# Generate output from the model\noutputs = model.generate(\n    inputs, \n    max_new_tokens=1024,\n    do_sample=False,\n    pad_token_id=tokenizer.eos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n)\n\n# Decode the output to text\noutput_text = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\nprint(\"Model Output:\", output_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:32:52.368355Z","iopub.execute_input":"2025-01-14T18:32:52.368630Z","iopub.status.idle":"2025-01-14T18:32:56.071616Z","shell.execute_reply.started":"2025-01-14T18:32:52.368610Z","shell.execute_reply":"2025-01-14T18:32:56.070776Z"}},"outputs":[{"name":"stdout","text":"Model Output: {\n  \"function_call\": \"get_weather\",\n  \"arguments\": {\n    \"location\": \"New York\"\n  }\n}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### Using OpenAI-style approach\nUsing this approach, we will automatically execute the function based on the model's instruction. We have to be more strict with the JSON output and more setup in general.","metadata":{}},{"cell_type":"code","source":"# Creating our JSON structure. \njson_input = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get current temperature for a given location.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"City and country e.g. Bogotá, Colombia\"\n                    }\n                },\n                \"required\": [\n                    \"location\"\n                ],\n                \"additionalProperties\": False\n            },\n            \"strict\": True\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calculate_sum\",\n            \"description\": \"Calculate the sum of two integers.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"a\": {\n                        \"type\": \"integer\",\n                        \"description\": \"First integer to add\"\n                    },\n                    \"b\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Second integer to add\"\n                    }\n                },\n                \"required\": [\n                    \"a\",\n                    \"b\"\n                ],\n                \"additionalProperties\": False\n            },\n            \"strict\": True\n        }\n    }\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:41:57.099291Z","iopub.execute_input":"2025-01-14T18:41:57.099565Z","iopub.status.idle":"2025-01-14T18:41:57.104776Z","shell.execute_reply.started":"2025-01-14T18:41:57.099544Z","shell.execute_reply":"2025-01-14T18:41:57.103787Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Our prompt to include the tools object\nprompt = f\"\"\"\nYou are an assistant that can call the following functions:\n\n{json.dumps(json_input, indent=4)}\n\nPlease provide a function call in JSON format when appropriate.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:41:58.957327Z","iopub.execute_input":"2025-01-14T18:41:58.957602Z","iopub.status.idle":"2025-01-14T18:41:58.961384Z","shell.execute_reply.started":"2025-01-14T18:41:58.957581Z","shell.execute_reply":"2025-01-14T18:41:58.960435Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"inputs = tokenizer.apply_chat_template(\n    [{'role': 'user', 'content': prompt}],\n    return_tensors=\"pt\"\n).to(model.device)\n\n# Generate output from the model\noutputs = model.generate(\n    inputs, \n    max_new_tokens=1024,\n    do_sample=False,\n    pad_token_id=tokenizer.eos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n)\n\n# Decode the output to text\noutput_text = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\nprint(\"Model Output:\", output_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:42:00.831403Z","iopub.execute_input":"2025-01-14T18:42:00.831677Z","iopub.status.idle":"2025-01-14T18:42:05.972715Z","shell.execute_reply.started":"2025-01-14T18:42:00.831656Z","shell.execute_reply":"2025-01-14T18:42:05.971653Z"}},"outputs":[{"name":"stdout","text":"Model Output: {\n  \"function\": \"get_weather\",\n  \"parameters\": {\n    \"location\": \"Bogota, Colombia\"\n  }\n}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Stage 3: Evaluation\nAfter testing out various approaches of facilitating function calling, one can create a sample dataset with `n` function examples with corresponding ground truth samples and compare the model's generated output versus the ground output. \n\nThere are many open-source LLMs with dedicated function calling features but one can emulate function calling through LLMs where function calling may not be accurately defined. ","metadata":{}}]}